{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"INDRA(Pytorch).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM3fJsPKUfc8uT+AbSJLx0y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"cVgOs67klLdg","executionInfo":{"status":"ok","timestamp":1635461494757,"user_tz":360,"elapsed":348209,"user":{"displayName":"Calvin Tai","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11662499921982332641"}}},"source":["!git clone https://github.com/etas/SynCAN.git &> /dev/null\n","!unzip ./SynCAN/\\*.zip -d ./SynCAN/. &> /dev/null\n","!rm ./SynCAN/*.zip &> /dev/null"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SkAZx1O_a54_","executionInfo":{"status":"ok","timestamp":1635461508200,"user_tz":360,"elapsed":13449,"user":{"displayName":"Calvin Tai","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11662499921982332641"}},"outputId":"37848668-285d-423f-a0ed-73d79450aab5"},"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","import matplotlib\n","matplotlib.use('Agg')\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","sns.set_style(\"darkgrid\")\n","import argparse\n","from pathlib import Path\n","import json\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm,trange\n","import time\n","import pickle\n","torch.manual_seed(1234)"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f2cd216d030>"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"fIMsvzOpeTnE","executionInfo":{"status":"ok","timestamp":1635461508201,"user_tz":360,"elapsed":18,"user":{"displayName":"Calvin Tai","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11662499921982332641"}}},"source":["class EarlyStopping(object):\n","    def __init__(self, mode='min', min_delta=0, patience=10, percentage=False):\n","        self.mode = mode\n","        self.min_delta = min_delta\n","        self.patience = patience\n","        self.best = None\n","        self.num_bad_epochs = 0\n","        self.is_better = None\n","        self._init_is_better(mode, min_delta, percentage)\n","\n","        if patience == 0:\n","            self.is_better = lambda a, b: True\n","            self.step = lambda a: False\n","\n","    def step(self, metrics):\n","        if self.best is None:\n","            self.best = metrics\n","            return False\n","\n","        if torch.isnan(metrics):\n","            return True\n","\n","        if self.is_better(metrics, self.best):\n","            self.num_bad_epochs = 0\n","            self.best = metrics\n","        else:\n","            self.num_bad_epochs += 1\n","\n","        if self.num_bad_epochs >= self.patience:\n","            return True\n","\n","        return False\n","\n","    def _init_is_better(self, mode, min_delta, percentage):\n","        if mode not in {'min', 'max'}:\n","            raise ValueError('mode ' + mode + ' is unknown!')\n","        if not percentage:\n","            if mode == 'min':\n","                self.is_better = lambda a, best: a < best - min_delta\n","            if mode == 'max':\n","                self.is_better = lambda a, best: a > best + min_delta\n","        else:\n","            if mode == 'min':\n","                self.is_better = lambda a, best: a < best - (\n","                            best * min_delta / 100)\n","            if mode == 'max':\n","                self.is_better = lambda a, best: a > best + (\n","                            best * min_delta / 100)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"zd1V66lcb-Zm","executionInfo":{"status":"ok","timestamp":1635461508202,"user_tz":360,"elapsed":17,"user":{"displayName":"Calvin Tai","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11662499921982332641"}}},"source":["# Here we define our model as a class\n","class INDRA_Enc(nn.Module):\n","\n","    def __init__(self, input_dim, enc_input_dim,\n","                enc_output_dim, num_layers, batch_size):\n","        super(INDRA_Enc, self).__init__()\n","        self.input_dim = input_dim #Number of singals per MSG ID\n","        self.enc_input_dim = enc_input_dim\n","        self.enc_output_dim = enc_output_dim\n","        self.num_layers = num_layers\n","        self.batch_size = batch_size\n","\n","        self.linear_enc = nn.Sequential(\n","                        nn.Linear(self.input_dim,self.enc_input_dim),\n","                        nn.Tanh())#,\n","                        # nn.Dropout(p=0.2))\n","        self.gru_enc = nn.GRU(self.enc_input_dim,self.enc_output_dim,\n","                                num_layers=self.num_layers)\n","        self.tanh_actv = nn.Tanh()\n","        self.drop_out = nn.Dropout(p=0.2)\n","\n","    def init_hidden(self, hidden_dim):\n","        return torch.zeros(self.num_layers, self.batch_size, hidden_dim)\n","\n","    def forward(self, input, hidden, drp_out):\n","        input = self.linear_enc(input)  #Making input ready for GRU encoder\n","        if drp_out:\n","            input = self.drop_out(input)\n","        enc_out , enc_hidden = self.gru_enc(input.view(len(input),\n","                                            self.batch_size,-1), hidden)\n","        enc_out = self.tanh_actv(enc_out)\n","        enc_hidden = self.tanh_actv(enc_hidden)\n","\n","        if drp_out:\n","            enc_out = self.drop_out(enc_out)\n","            enc_hidden = self.drop_out(enc_hidden)\n","\n","        return enc_out, enc_hidden"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"pejEJscRcEpx","executionInfo":{"status":"ok","timestamp":1635461508202,"user_tz":360,"elapsed":15,"user":{"displayName":"Calvin Tai","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11662499921982332641"}}},"source":["class INDRA_Dec(nn.Module):\n","\n","    def __init__(self, dec_input_dim, dec_output_dim,\n","                linear_output_dim, num_layers, batch_size):\n","        super(INDRA_Dec, self).__init__()\n","        self.dec_input_dim = dec_input_dim  #Encoder output dimensions\n","        self.dec_output_dim = dec_output_dim\n","        self.linear_output_dim = linear_output_dim #Number of signals per MSG ID\n","        self.num_layers = num_layers\n","        self.batch_size = batch_size\n","\n","        self.gru_dec = nn.GRU(self.dec_input_dim,self.dec_output_dim,\n","                                num_layers=self.num_layers)\n","        self.tanh_actv = nn.Tanh()\n","        self.linear_dec = nn.Sequential(\n","                        nn.Linear(self.dec_output_dim,self.linear_output_dim),\n","                        nn.Tanh())#,\n","                        # nn.Dropout(p=0.2))\n","        self.drop_out = nn.Dropout(p=0.2)\n","\n","    def init_hidden(self, hidden_dim):\n","        return torch.zeros(self.num_layers, self.batch_size, hidden_dim)\n","\n","    def forward(self, input, hidden, drp_out):\n","        dec_out , dec_hidden = self.gru_dec(input.view(len(input),\n","                                            self.batch_size,-1), hidden)\n","        dec_out = self.tanh_actv(dec_out)\n","        dec_hidden = self.tanh_actv(dec_hidden)\n","        if drp_out:\n","            dec_out = self.drop_out(dec_out)\n","            dec_hidden = self.drop_out(dec_hidden)\n","\n","        dec_out = self.linear_dec(dec_out)\n","\n","        return dec_out"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"m4mJYqYVcG0o","executionInfo":{"status":"ok","timestamp":1635461508203,"user_tz":360,"elapsed":16,"user":{"displayName":"Calvin Tai","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11662499921982332641"}}},"source":["class INDRA_ED_2L(nn.Module):\n","    def __init__(self, input_dim, linear_out_dim, context_dim, num_layers, batch_size):\n","        super(INDRA_ED_2L, self).__init__()\n","        self.encoder = INDRA_Enc(input_dim, linear_out_dim, context_dim, num_layers, batch_size)\n","        self.decoder = INDRA_Dec(context_dim, linear_out_dim, input_dim, num_layers, batch_size)\n","        self.linear_out_dim = linear_out_dim\n","\n","    def forward(self, input, hidden1, hidden2, drp_out):\n","        enc_out, enc_hidden = self.encoder(input, hidden1, drp_out)\n","        dec_out = self.decoder(enc_out, hidden2, drp_out)\n","\n","        return dec_out"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"at3_y9yGcIlq","executionInfo":{"status":"ok","timestamp":1635461508204,"user_tz":360,"elapsed":16,"user":{"displayName":"Calvin Tai","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11662499921982332641"}}},"source":["# def csv2df(dir_path):\n","#     all_files = [file for file in sorted(dir_path.glob('train_*.zip'))]\n","\n","#     ## TODO: Maybe create a new dataset using the below steps and avoid\n","#     ## repeating these steps every time the program is ran\n","\n","#     all_cols = ['Label','ID', 'Time', 'Signal1','Signal2','Signal3','Signal4']\n","#     df_list = []\n","#     for f in tqdm(all_files,bar_format=\"{l_bar}{bar}|\", ncols=50, desc = 'Loading dataset'):\n","#         if f == [all_files[0]]:\n","#             tmp_df = pd.read_csv(f,sep=',',header=0)\n","#         else:\n","#             tmp_df = pd.read_csv(f,sep=',',header=None,names = all_cols)\n","#         df_list.append(tmp_df)\n","\n","#     df = pd.concat(df_list, sort=False, ignore_index=False, axis=0)\n","\n","#     return df"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"EBw-t5Y8cOla","executionInfo":{"status":"ok","timestamp":1635461508205,"user_tz":360,"elapsed":16,"user":{"displayName":"Calvin Tai","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11662499921982332641"}}},"source":["# def prepare_dataset(dir_path, msg_id):\n","\n","#     ## Import data to pandas df from csv\n","#     data_frame = csv2df(dir_path)\n","\n","#     ## Preprocess dataset to make it ready for the model\n","#     #Filtering out other msg IDs from the data set\n","#     data_frame = data_frame[:][data_frame.ID==msg_id]\n","\n","#     #dropping all columns with all NaN\n","#     data_frame = data_frame.dropna(axis=1,how='all')\n","\n","#     #checking if DF corresponds to only one MSG ID\n","#     assert len(data_frame.ID.unique()) == 1\n","\n","#     #checking if there are any NaNs\n","#     assert data_frame.isnull().values.any() == False\n","\n","#     #Removing columns other than the signal data (3rd Column to the end)\n","#     data_frame = data_frame.iloc[:,3:len(data_frame.columns)]\n","\n","#     return data_frame"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"zDOkZw-km6Y6","executionInfo":{"status":"ok","timestamp":1635461508206,"user_tz":360,"elapsed":16,"user":{"displayName":"Calvin Tai","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11662499921982332641"}}},"source":["def csv2df(dir_path):\n","    data_frames = []\n","    csv_path = dir_path + \"/train_1.csv\"\n","    df_temp = pd.read_csv(csv_path)\n","    data_frames.append(df_temp)\n","    for i in range(2, 5):\n","        csv_path = dir_path + \"/train_\" + str(i) + \".csv\"\n","        df_temp = pd.read_csv(csv_path, header=None, names=[\"Label\",  \"Time\", \"ID\", \"Signal1\",  \"Signal2\",  \"Signal3\",  \"Signal4\"])\n","        data_frames.append(df_temp)\n","    df = pd.concat(data_frames)\n","    return df\n","\n","def prepare_dataset(msg_id):\n","    df = csv2df(\"/content/SynCAN\")\n","    df = df[:][df.ID==msg_id]\n","    df = df.dropna(axis=1,how='all')\n","    assert len(df.ID.unique()) == 1\n","    assert df.isnull().values.any() == False\n","    df = df.iloc[:,3:len(df.columns)]\n","    return df"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"C7M_WtNrcTyG","executionInfo":{"status":"ok","timestamp":1635461508207,"user_tz":360,"elapsed":17,"user":{"displayName":"Calvin Tai","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11662499921982332641"}}},"source":["class SynCAN_Dataset(Dataset):\n","\n","    def __init__(self,data,ss_len):\n","        self.data = data\n","        self.num_samples = len(self.data)\n","        self.ss_len = ss_len    #ss_len is the rolling window size\n","\n","    def __len__(self):\n","        return len(self.data) - self.ss_len\n","\n","    def __getitem__(self,idx):\n","        return torch.tensor(self.data[idx:idx+self.ss_len].values.astype(float)).float()\n","        # _y = torch.tensor(self.data[idx+self.ss_len:idx+self.ss_len+1].values).float()\n","        # _y = torch.tensor(self.data[idx+1:idx+self.ss_len+1].values).float()\n","\n","        # return [_x, _y]"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"APvsntl2cVin","executionInfo":{"status":"ok","timestamp":1635461508208,"user_tz":360,"elapsed":18,"user":{"displayName":"Calvin Tai","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11662499921982332641"}}},"source":["def update_best_stats(stats_dict,ep,Tr_loss,Vl_loss):\n","\n","    Tr_loss_array = np.asarray(Tr_loss, dtype=np.float32)\n","    Vl_loss_array = np.asarray(Vl_loss, dtype=np.float32)\n","\n","    stats_dict['epoch'] = ep\n","    stats_dict['running_train_loss'] = np.sum(Tr_loss_array)\n","    stats_dict['min_train_loss'] = np.min(Tr_loss_array)\n","    stats_dict['max_train_loss'] = np.max(Tr_loss_array)\n","    stats_dict['avg_train_loss'] = np.mean(Tr_loss_array)\n","    stats_dict['99_999p_train_loss'] = np.percentile(Tr_loss_array,99.999)\n","    stats_dict['99_99p_train_loss'] = np.percentile(Tr_loss_array,99.99)\n","    stats_dict['99_9p_train_loss'] = np.percentile(Tr_loss_array,99.9)\n","    stats_dict['99p_train_loss'] = np.percentile(Tr_loss_array,99)\n","    stats_dict['90p_train_loss'] = np.percentile(Tr_loss_array,90)\n","    stats_dict['median_train_loss'] = np.percentile(Tr_loss_array,50)\n","\n","    stats_dict['running_val_loss'] = np.sum(Vl_loss_array)\n","    stats_dict['min_val_loss'] = np.min(Vl_loss_array)\n","    stats_dict['max_val_loss'] = np.max(Vl_loss_array)\n","    stats_dict['avg_val_loss'] = np.mean(Vl_loss_array)\n","    stats_dict['99_999p_val_loss'] = np.percentile(Vl_loss_array,99.999)\n","    stats_dict['99_99p_val_loss'] = np.percentile(Vl_loss_array,99.99)\n","    stats_dict['99_9p_val_loss'] = np.percentile(Vl_loss_array,99.9)\n","    stats_dict['99p_val_loss'] = np.percentile(Vl_loss_array,99)\n","    stats_dict['90p_val_loss'] = np.percentile(Vl_loss_array,90)\n","    stats_dict['92p_val_loss'] = np.percentile(Vl_loss_array,92)\n","    stats_dict['94p_val_loss'] = np.percentile(Vl_loss_array,94)\n","    stats_dict['95p_val_loss'] = np.percentile(Vl_loss_array,95)\n","    stats_dict['96p_val_loss'] = np.percentile(Vl_loss_array,96)\n","    stats_dict['98p_val_loss'] = np.percentile(Vl_loss_array,98)\n","    stats_dict['median_val_loss'] = np.percentile(Vl_loss_array,50)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Wj5CUMhcXd2","executionInfo":{"status":"ok","timestamp":1635461508209,"user_tz":360,"elapsed":18,"user":{"displayName":"Calvin Tai","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11662499921982332641"}}},"source":["def setup_folders(*args):\n","    for folder in args:\n","        if not folder.exists():\n","            folder.mkdir(mode=0o775, parents=False, exist_ok=False)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"O0kbfwzkcZeH","executionInfo":{"status":"ok","timestamp":1635461784827,"user_tz":360,"elapsed":1203,"user":{"displayName":"Calvin Tai","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11662499921982332641"}}},"source":["def main(args):\n","    start_time = time.time()\n","\n","    if args[\"config\"] not in ['ED_2L' , 'ED_2L_AL', 'DR_ED_2L', 'DR_ED_2L_AL']:\n","        print(\"Invalid settings configuration\")\n","        assert False\n","    else:\n","        if \"_AL\" in args[\"config\"]:\n","            lr_factor = 10\n","        else:\n","            lr_factor = 1\n","\n","        settings_config = \"ED_2L\"\n","\n","    drp_out = 1 if \"DR_\" in args[\"config\"] else 0\n","\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","    ### Checking for existence of required folders\n","    output_dir = args[\"output_model\"]\n","    if not output_dir.exists():\n","        output_dir.mkdir(mode=0o775, parents=False, exist_ok=False)\n","\n","    plot_dir = args[\"plot_dir\"]\n","    if not plot_dir.exists():\n","        plot_dir.mkdir(mode=0o775, parents=False, exist_ok=False)\n","\n","    stats_dir = args[\"stats_dir\"]\n","    if not stats_dir.exists():\n","        stats_dir.mkdir(mode=0o775, parents=False, exist_ok=False)\n","\n","    ### loading settings from json\n","    # with open(str(args[\"settings\"]),'r') as settings_file:\n","    #     settings = json.load(settings_file)\n","\n","    ### hyper parameters\n","    settings = args[\"settings\"]\n","    # TODO: Prune the parameters\n","    num_layers_ = settings[settings_config]['num_layers']\n","    num_epochs = settings[settings_config]['num_epochs']\n","    batch_size = settings[settings_config]['batch_size']\n","    sub_seq_len = settings[settings_config]['sub_seq_len']\n","    train_split = settings[settings_config]['train_split']\n","    val_split = settings[settings_config]['val_split']\n","    learning_rate = lr_factor * settings[settings_config]['learning_rate']\n","    context_dim = settings[settings_config]['context_dim']\n","    lin_out_dim = settings[settings_config]['lin_out_dim']\n","    L_func = settings[settings_config]['L_func']\n","    patience_ = settings[settings_config]['patience']\n","    optimizer = settings[settings_config]['optimizer']\n","    test_str = str(args[\"config\"])\n","\n","    print('Settings:')\n","    print('--------')\n","    print('Msg ID: id{0}'.format(args[\"msg_id\"]))\n","    print('num_epochs {0}'.format(num_epochs))\n","    print('batch_size {0}'.format(batch_size))\n","    print('sub_seq_len {0}'.format(sub_seq_len))\n","    print('learning_rate {0}'.format(learning_rate))\n","    print('patience_ {0}'.format(patience_))\n","    print('dropout {0}'.format(drp_out))\n","    print('configuration {0}'.format(args[\"config\"]))\n","\n","    ### Pre-processing data_set\n","    msg_id = 'id' + str(args[\"msg_id\"])\n","    directory = Path(args[\"dataset\"])\n","    data_set = prepare_dataset(msg_id)\n","    num_features = len(data_set.columns)\n","\n","    ### Pruning data_set to get integer number of sub_sequences\n","    num_samples_org = len(data_set.index)\n","    num_ss = num_samples_org//sub_seq_len\n","    data_set = data_set.iloc[:(num_ss*sub_seq_len)]\n","\n","    ### Computing train and validation data sizes (in #samples)\n","    num_samples_adj = len(data_set.index)   # num adjusted samples\n","    train_size = int(num_samples_adj*train_split)  # data samples used for training\n","    val_size = num_samples_adj - train_size # data samples used for validation\n","    # train_size = int(num_samples_adj*0.09)\n","    # val_size = int(num_samples_adj*0.01)\n","\n","    ### Creating pyTorch datasets\n","    train_data = SynCAN_Dataset(data_set[:train_size],sub_seq_len)\n","    val_data = SynCAN_Dataset(data_set[train_size:train_size+val_size],sub_seq_len)\n","\n","    ### Creating a DataLoaders\n","    train_loader = DataLoader(dataset=train_data,batch_size=\n","                                    batch_size,shuffle=False, drop_last=\n","                                    True if (train_size % batch_size != 0) else False)\n","\n","    val_loader = DataLoader(dataset=val_data,batch_size=\n","                                    batch_size,shuffle=False,drop_last=\n","                                    True if (val_size % batch_size != 0) else False)\n","\n","    # available_ram = psutil.virtual_memory().available / (1024 * 1024 * 1024)\n","\n","    ## TODO: Make the variable names consistant\n","    model = INDRA_ED_2L(input_dim=num_features, linear_out_dim = lin_out_dim, context_dim= context_dim,\n","                    num_layers= num_layers_, batch_size= batch_size).to(device)\n","\n","    if L_func == \"BCE\":\n","        print(\"BCE loss\")\n","        loss_fn = torch.nn.BCELoss().to(device)    ## TODO: Change the loss to the custom loss defined in the paper\n","    elif L_func == \"MSE\":\n","        print(\"MSE loss\")\n","        loss_fn = torch.nn.MSELoss().to(device)\n","    elif L_func == \"RMSE\":\n","        print(\"RMSE loss\")\n","        loss_fn = torch.nn.MSELoss().to(device) #sqrt is computed in the training loop\n","    else:\n","        print(\"Invalid loss function\")\n","        assert False\n","\n","    if optimizer == \"ADAM\":\n","        optimiser = torch.optim.Adam(model.parameters(),lr=learning_rate)\n","    elif optimizer == \"SGD\":\n","        optimiser = torch.optim.SGD(model.parameters(),lr=learning_rate)\n","    else:\n","        print(\"Invalid optimiser\")\n","        assert False\n","\n","    if \"_AL\" in args[\"config\"]:\n","        scheduler = ReduceLROnPlateau(optimiser, mode='min', patience=5)\n","\n","    running_loss_list = []\n","    val_loss_list = []\n","    avg_train_loss = []\n","    avg_val_loss = []\n","    es_epoch = 0\n","    best_val_running_loss = None\n","\n","    stats_columns = ['epoch',\n","                    'running_train_loss','min_train_loss','max_train_loss','avg_train_loss',\n","                    'running_val_loss','min_val_loss','max_val_loss','avg_val_loss',\n","                    'best_epoch']\n","\n","    best_stats_keys = ['epoch',\n","                 'running_train_loss','min_train_loss','max_train_loss','avg_train_loss',\n","                 '99_999p_train_loss', '99_99p_train_loss','99_9p_train_loss','99p_train_loss','90p_train_loss', 'median_train_loss',\n","                 'running_val_loss','min_val_loss','max_val_loss','avg_val_loss',\n","                 '99_999p_val_loss', '99_99p_val_loss','99_9p_val_loss','99p_val_loss','90p_val_loss', 'median_val_loss']\n","\n","    best_stats = dict.fromkeys(best_stats_keys)\n","\n","    stats_list = [dict.fromkeys(stats_columns) for i in range(num_epochs)]\n","\n","    fname_prefix = str(msg_id + '_' + L_func + '_' + optimizer + '_' + test_str + '_L' + str(num_layers_))\n","\n","    es = EarlyStopping(patience = patience_)\n","\n","    ## plotting settigs\n","    font = {\n","        'weight' : 'bold'}#,\n","        # 'size'   : 18}\n","    matplotlib.rc('font', **font)\n","\n","    # ########################### #\n","    #   Train and validate model  #\n","    # ########################### #\n","\n","    for epoch in range(num_epochs):\n","\n","        print(\"Epoch: \", epoch+1 , \"/\" , num_epochs)\n","        # Initialise hidden state\n","        # model.hidden1 = model.init_hidden(512).to(device)\n","        # model.hidden2 = model.init_hidden(num_features).to(device)\n","        # enc_model.hidden = tuple(i.to(device) for i in model.hidden)\n","        # dec_model.hidden = tuple(i.to(device) for i in model.hidden)\n","        hidden1 = torch.zeros(num_layers_,batch_size,context_dim).to(device)\n","        hidden2 = torch.zeros(num_layers_,batch_size,lin_out_dim).to(device)\n","        running_loss = 0\n","        train_itr_loss = []\n","        val_itr_loss = []\n","        val_running_loss = 0\n","\n","        # stats_dict = dict.fromkeys(stats_columns)\n","\n","        ### Training ###\n","        model.train()\n","\n","        for itr, input_batch in enumerate(tqdm(train_loader,\n","            bar_format=\"{l_bar}{bar}|\", ncols=50, desc='Training'),0):\n","            # bar_format=\"{l_bar}{bar}|{n_fmt}/{total_fmt}\", ncols=50, desc='Train'),0):\n","            # input_batch = input_batch.reshape(sub_seq_len,batch_size,num_features).to(device)\n","            input_batch = input_batch.permute(1,0,2).to(device)\n","            # pred_batch = pred_batch.reshape(batch_size,num_features).to(device)\n","            # pred_batch = pred_batch.reshape(sub_seq_len*batch_size,num_features).to(device)\n","\n","            # Clear stored gradient\n","            model.zero_grad()\n","\n","            # forward pass\n","            reconstruction = model(input_batch, hidden1, hidden2, drp_out)\n","\n","            # Compute loss\n","            loss = loss_fn(reconstruction, input_batch)\n","            if L_func == \"RMSE\":\n","                loss= torch.sqrt(loss)\n","\n","            # Zero out gradient, else they will accumulate between epochs\n","            optimiser.zero_grad()\n","\n","            # Backward pass\n","            loss.backward(retain_graph=False)\n","\n","            # Update parameters\n","            optimiser.step()\n","\n","            # accumulating loss\n","            running_loss += loss.item()\n","            train_itr_loss.append(loss.item())\n","\n","        running_loss_list.append(running_loss)\n","        avg_train_loss.append(sum(train_itr_loss)/len(train_itr_loss))\n","\n","        ### evaluate performance on validation set\n","        model.eval()\n","        val_hidden1 = torch.zeros(num_layers_,batch_size,context_dim).to(device)\n","        val_hidden2 = torch.zeros(num_layers_,batch_size,lin_out_dim).to(device)\n","\n","        with torch.no_grad():\n","            for val_itr, val_input_batch in enumerate(tqdm(val_loader,\n","                            bar_format=\"{l_bar}{bar}|\", ncols=50, desc='Validaiton'),0):\n","                            # bar_format=\"{l_bar}{bar}|{n_fmt}/{total_fmt}\", ncols=50, desc='Val'),0):\n","                # val_input_batch = val_input_batch.reshape(sub_seq_len,batch_size,num_features).to(device)\n","                val_input_batch = val_input_batch.permute(1,0,2).to(device)\n","                # val_pred_batch = val_pred_batch.reshape(batch_size,num_features).to(device)\n","                # val_pred_batch = val_pred_batch.reshape(sub_seq_len*batch_size,num_features).to(device)\n","\n","                val_recons = model(val_input_batch, val_hidden1, val_hidden2, drp_out)\n","\n","                val_loss = loss_fn(val_recons,val_input_batch)\n","                if L_func == \"RMSE\":\n","                    val_loss= torch.sqrt(val_loss)\n","\n","                val_running_loss += val_loss.item()\n","                val_itr_loss.append(val_loss.item())\n","\n","        val_loss_list.append(val_running_loss)\n","        avg_val_loss.append(sum(val_itr_loss)/len(val_itr_loss))\n","        if \"_AL\" in args[\"config\"]:\n","            scheduler.step(val_running_loss)\n","\n","        print(\"\")\n","        print(\"Train loss: \", running_loss)\n","        print(\"Validation loss: \", val_running_loss)\n","        print(\"Avg train loss: \", (sum(train_itr_loss)/len(train_itr_loss)))\n","        print(\"Avg validation loss: \", (sum(val_itr_loss)/len(val_itr_loss)))\n","        print(\"\")\n","\n","        stats_list[epoch]['epoch'] = epoch\n","        stats_list[epoch]['running_train_loss'] = running_loss\n","        stats_list[epoch]['min_train_loss'] = min(train_itr_loss)\n","        stats_list[epoch]['min_train_loss'] = max(train_itr_loss)\n","        stats_list[epoch]['avg_train_loss'] = sum(train_itr_loss)/len(train_itr_loss)\n","        stats_list[epoch]['running_val_loss'] = val_running_loss\n","        stats_list[epoch]['min_val_loss'] = min(val_itr_loss)\n","        stats_list[epoch]['min_val_loss'] = max(val_itr_loss)\n","        stats_list[epoch]['avg_val_loss'] = sum(val_itr_loss)/len(val_itr_loss)\n","        stats_list[epoch]['best_epoch'] = 0\n","\n","        # stats_dict['epoch'] = epoch\n","        # stats_dict['running_train_loss'] = running_loss\n","        # stats_dict['min_train_loss'] = min(train_itr_loss)\n","        # stats_dict['min_train_loss'] = max(train_itr_loss)\n","        # stats_dict['avg_train_loss'] = sum(train_itr_loss)/len(train_itr_loss)\n","        # stats_dict['running_val_loss'] = val_running_loss\n","        # stats_dict['min_val_loss'] = min(val_itr_loss)\n","        # stats_dict['min_val_loss'] = max(val_itr_loss)\n","        # stats_dict['avg_val_loss'] = sum(val_itr_loss)/len(val_itr_loss)\n","        # stats_dict['best_epoch'] = 0\n","        # stats_df.loc[len(stats_df)] = stats_dict\n","\n","        ### Checkpointing and EarlyStopping\n","        if best_val_running_loss is not None:\n","            if val_running_loss < best_val_running_loss:\n","                best_val_running_loss = val_running_loss\n","                model_name = str(Path(output_dir,fname_prefix + '.pt'))\n","                torch.save(model.state_dict(), model_name)\n","                stats_list[epoch]['best_epoch'] = 1\n","                update_best_stats(best_stats,epoch, train_itr_loss, val_itr_loss)\n","                assert best_stats['epoch'] == epoch #ensuring correct update\n","        else:\n","            best_val_running_loss = val_running_loss\n","            model_name = str(Path(output_dir,fname_prefix + '.pt'))\n","            torch.save(model.state_dict(), model_name)\n","            stats_list[epoch]['best_epoch'] = 1\n","            update_best_stats(best_stats,epoch, train_itr_loss, val_itr_loss)\n","            assert best_stats['epoch'] == epoch #ensuring correct update\n","\n","        if es.step(torch.tensor([val_running_loss])):\n","            print(\"Early Stopping the training!\")\n","            es_epoch = epoch - patience_\n","            break\n","\n","    if es_epoch == 0:   #Never hit early stopping\n","        es_epoch = epoch\n","\n","    ## save the list to a df\n","    stats_df = pd.DataFrame(stats_list)\n","    ## save df to disk\n","    df_fname = str(Path(stats_dir,fname_prefix + '_train' + '.pt'))\n","    stats_df.to_pickle(df_fname)\n","\n","    #Save the best_stats to a pickle file\n","    best_stats_fname = str(Path(stats_dir,fname_prefix + '_train_best' + '.pt'))\n","    with open(best_stats_fname, 'wb') as handle:\n","        pickle.dump(best_stats, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","    ## Saving the model ###\n","    model_name = str(Path(output_dir, fname_prefix + '_Last' + '.pt'))\n","    torch.save(model.state_dict(), model_name)\n","\n","    plt.figure()\n","    plt.title(\"Running Loss\", fontweight='bold', fontsize='12')\n","    plt.plot(running_loss_list,label='Training')\n","    plt.plot(val_loss_list, label = 'Validation')\n","    plt.axvline(es_epoch, label = 'EarlyStopping',color='red',linestyle='--')\n","    plt.xlabel('epochs', fontweight='bold', fontsize='12')\n","    plt.ylabel('Loss', fontweight='bold', fontsize='12')\n","    plt.legend()\n","    plt_fname = str(Path(plot_dir, fname_prefix +'_running_loss' + '.png'))\n","    plt.savefig(plt_fname, dpi=500)\n","\n","    plt.figure()\n","    plt.title(\"Average Loss\", fontweight='bold', fontsize='12')\n","    plt.plot(avg_train_loss,label='Training')\n","    plt.plot(avg_val_loss, label = 'Validation')\n","    plt.axvline(es_epoch, label = 'EarlyStopping',color='red',linestyle='--')\n","    plt.xlabel('epochs', fontweight='bold', fontsize='12')\n","    plt.ylabel('Loss', fontweight='bold', fontsize='12')\n","    plt.legend()\n","    plt_fname = str(Path(plot_dir, fname_prefix +'_avg_loss' + '.png'))\n","    plt.savefig(plt_fname, dpi=500)\n","\n","    print(\"\\n\\n--- Total time: %s seconds ---\" % (time.time() - start_time))"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"krO44BKZeK0t","executionInfo":{"status":"ok","timestamp":1635461798876,"user_tz":360,"elapsed":369,"user":{"displayName":"Calvin Tai","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11662499921982332641"}}},"source":["### Simulate args\n","settings = {\n","    \"LinDec\" : {\n","        \"num_layers\" : 1,\n","        \"num_epochs\" : 200,\n","        \"batch_size\" : 128,\n","        \"sub_seq_len\": 20,\n","        \"train_split\": 0.90,\n","        \"val_split\": 0.10,\n","        \"save_epochs\": 3,\n","        \"patience\": 8,\n","        \"learning_rate\" : 0.0001,\n","        \"context_dim\" : 512,\n","        \"L_func\" : \"MSE\",\n","        \"optimizer\" : \"ADAM\",\n","        \"max_plt_count\" : 8,\n","        \"IS_config\" : \"avg\"\n","\n","    },\n","    \"ED_2L\" : {\n","        \"num_layers\" : 1,\n","        \"num_epochs\" : 200,\n","        \"batch_size\" : 128,\n","        \"sub_seq_len\": 20,\n","        \"train_split\": 0.90,\n","        \"val_split\": 0.10,\n","        \"save_epochs\": 3,\n","        \"patience\": 8,\n","        \"learning_rate\" : 0.0001,\n","        \"lin_out_dim\" : 128,\n","        \"context_dim\" : 64,\n","        \"L_func\" : \"MSE\",\n","        \"optimizer\" : \"ADAM\",\n","        \"max_plt_count\" : 8,\n","        \"IS_config\" : \"avg\"\n","\n","    },\n","    \"1L_ED\" : {\n","        \"num_layers\" : 1,\n","        \"num_epochs\" : 200,\n","        \"batch_size\" : 128,\n","        \"sub_seq_len\": 20,\n","        \"train_split\": 0.90,\n","        \"val_split\": 0.10,\n","        \"save_epochs\": 3,\n","        \"patience\": 8,\n","        \"learning_rate\" : 0.0001,\n","        \"lin_out_dim\" : 128,\n","        \"context_dim\" : 64,\n","        \"L_func\" : \"MSE\",\n","        \"optimizer\" : \"ADAM\",\n","        \"max_plt_count\" : 8,\n","        \"IS_config\" : \"avg\"\n","\n","    },\n","    \"attacks\" : {\n","        \"no_attack\" : 0,\n","        \"plateau\" : 1,\n","        \"continuous\" : 2,\n","        \"playback\" : 3,\n","        \"suppress\" : 4,\n","        \"flooding\" : 5\n","    },\n","}\n","args = {}\n","args[\"settings\"] = settings\n","args[\"dataset\"] = Path(Path.home(), 'SynCAN')\n","args[\"output_model\"] = Path(Path.cwd(),'output_model')\n","args[\"plot_dir\"] = Path(Path.cwd(),'plots')\n","args[\"stats_dir\"] = Path(Path.cwd(),'stats')\n","args[\"msg_id\"] = \"2\"\n","args[\"config\"] = \"ED_2L\"\n","args[\"resume\"] = False\n","# main(args)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"rsQDnkRTHwCw"},"source":[""],"execution_count":null,"outputs":[]}]}